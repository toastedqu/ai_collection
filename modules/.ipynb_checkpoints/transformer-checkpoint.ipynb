{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c24cf6a0",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801696c2",
   "metadata": {},
   "source": [
    "<center><img src=\"../images/attention_is_all_you_need.png\" width=50% height=60% /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc48b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0966a5b",
   "metadata": {},
   "source": [
    "## Core functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b826f0a",
   "metadata": {},
   "source": [
    "### Input/Output Embedding\n",
    "\n",
    "1. A sequence of texts is converted into a sequence of token ids (i.e., the position of the word in the dictionary)\n",
    "2. The sequence of token ids is converted into a matrix of one-hot vectors of shape *max_seq_len* $\\times$ *vocab_size*).\n",
    "3. The matrix is transformed into embeddings of shape *max_seq_len* $\\times$ *emb_size* through a learnable weight matrix of shape *vocab_size* $\\times$ *emb_size*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b2443a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        self.emb = nn.Embedding(vocab_size, emb_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - x: tokenized seqs of shape [batch_size, max_seq_len]\n",
    "        \n",
    "        Returns\n",
    "        - embeddings: of shape [batch_size, max_seq_len, emb_size]\n",
    "        \"\"\"\n",
    "        return self.emb(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b297be05",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b587b93c",
   "metadata": {},
   "source": [
    "Any other positional encoding function may apply, but the original paper uses the following waveforms:\n",
    "\n",
    "<center>\n",
    "$\n",
    "\\begin{align*}\n",
    "PE(pos,i_{even})&=\\sin\\left(\\frac{pos}{10000^{\\frac{i}{d_{model}}}}\\right)\\\\\n",
    "PE(pos,i_{odd})&=\\cos\\left(\\frac{pos}{10000^{\\frac{i-1}{d_{model}}}}\\right)\\\\\n",
    "\\end{align*}\n",
    "$\n",
    "</center>\n",
    "\n",
    "where\n",
    "- $pos$: position\n",
    "- $i$: dimension\n",
    "\n",
    "The original authors mentioned the following reasons of using the waveforms:\n",
    "- It would allow the model to easily learn to attend by relative positions because $PE(pos+k)$ can always be written as a linear function of $PE(pos)$ given any $k$.\n",
    "- It may allow the model to extrapolate to seq lengths longer than the ones encountered in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "31b67d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PositionalEncoding(max_seq_len, emb_size):\n",
    "    # get i_evens since i_odds will also use the same i_evens in the calculation\n",
    "    i_even = torch.arange(0, emb_size, 2).float()\n",
    "\n",
    "    # get positions, reshape to have 2d encoding\n",
    "    pos = torch.arange(max_seq_len).reshape(max_seq_len, 1)\n",
    "\n",
    "    # get the varioble inside sin and cos\n",
    "    x = pos/torch.pow(10000, i_even/emb_size)\n",
    "\n",
    "    # calculate and stack. flatten to match the final dimension\n",
    "    PEs = torch.stack([torch.sin(x), torch.cos(x)], dim=2)\n",
    "    PE = torch.flatten(PEs, start_dim=1, end_dim=2)\n",
    "    return PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d2b8389c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
       "        [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
       "        [ 0.9093, -0.4161,  0.0200,  0.9998],\n",
       "        [ 0.1411, -0.9900,  0.0300,  0.9996],\n",
       "        [-0.7568, -0.6536,  0.0400,  0.9992]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PositionalEncoding(5, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13116b66",
   "metadata": {},
   "source": [
    "### Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712bd7ba",
   "metadata": {},
   "source": [
    "There is no magic behind the fancy name \"self-attention\". It literally means automatically attending each word to each other and see their similarity/differences. \n",
    "\n",
    "However, the real magic comes with the Scaled Dot-Product Attention. We all know how it works, but we do not know why it works.\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q,K,V)=\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "1. Prepare 3 vectors for each word (i.e., 3 matrices):\n",
    "    - Q (Query): What you are looking for\n",
    "    - K (Key): The clues that guide you to your match\n",
    "    - V (Value): The match\n",
    "2. Calculate the attention scores between the queries and the keys using a dot product.\n",
    "    - The query vector for a specific word is multiplied with the key vector of every single word in the sequence.\n",
    "3. Scale the attention scores by the square root of the dimensionality of the key vectors to stabilize computation.\n",
    "    - $\\frac{QK^T}{\\sqrt{d_k}}$ is of similar scales/varainces as $Q$ and $K$, while $QK^T$ is much larger (thus much higher variances). Easily verifiable through testing.\n",
    "4. Softmax the scaled attention scores into probabilities.\n",
    "    - The probability means how much each column word attends to each row word.\n",
    "5. Multiply/Weight the values.\n",
    "    - For each row word, return a weighted sum of the values of the column words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "18b73564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScaledDotProductAttention(Q, K, V, masked=False):\n",
    "    max_seq_len, d_k = Q.shape\n",
    "    d_v = V.shape[1]\n",
    "    scores = torch.mm(Q, torch.t(K)) / math.sqrt(d_k)\n",
    "    if masked:\n",
    "        mask = torch.tril(torch.ones((max_seq_len, max_seq_len)))\n",
    "        mask[mask==0] = float(\"-Inf\")\n",
    "        mask[mask==1] = 0.\n",
    "        scores += mask\n",
    "    return torch.mm(F.softmax(scores, dim=1), V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e69321e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len, d_k, d_v = 5,4,3\n",
    "Q = torch.rand(max_seq_len, d_k)\n",
    "K = torch.rand(max_seq_len, d_k)\n",
    "V = torch.rand(max_seq_len, d_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7cab07a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3604, 0.0303, 0.5765],\n",
       "        [0.2773, 0.1038, 0.6105],\n",
       "        [0.3626, 0.3178, 0.6567],\n",
       "        [0.3730, 0.4145, 0.5764],\n",
       "        [0.3528, 0.3547, 0.4868]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa = ScaledDotProductAttention(Q, K, V, masked=True)\n",
    "sa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b138dac",
   "metadata": {},
   "source": [
    "### Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2106fe",
   "metadata": {},
   "source": [
    "If you have multiple modules that does the attention function above separately, and you concatenate them, you have multi-head attention:\n",
    "<center>\n",
    "$\n",
    "\\begin{align}\n",
    "\\text{MultiHead(Q,K,V)}&=\\text{Concat}(\\text{head}_1,\\cdots,\\text{head}_h)W^O\\\\\n",
    "\\text{head}_i&=\\text{Attention}(QW_i^Q,KW_i^K,VW_i^V)\n",
    "\\end{align}\n",
    "$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f75e7d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_dim, n_heads, masked=False):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.masked = masked\n",
    "        self.head_dim = hidden_dim // n_heads    # assume d_k == d_v\n",
    "        self.qkv = nn.Linear(emb_size, 3*hidden_dim)\n",
    "        self.linear = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "    def ScaledDotProductAttention(self, Q, K, V):\n",
    "        max_seq_len, d_k = Q.shape[-2:]\n",
    "        d_v = V.shape[1]\n",
    "        scores = torch.matmul(Q, K.transpose(-1,-2)) / math.sqrt(d_k)\n",
    "        if self.masked:\n",
    "            mask = torch.tril(torch.ones((max_seq_len, max_seq_len)))\n",
    "            mask[mask==0] = float(\"-Inf\")\n",
    "            mask[mask==1] = 0.\n",
    "            scores += mask\n",
    "        return torch.matmul(F.softmax(scores, dim=1), V)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, max_seq_len, _ = x.size()       # (batch_size, max_seq_len, emb_size)\n",
    "        x = self.qkv(x)                             # (batch_size, max_seq_len, 3*hidden_dim)\n",
    "        x = x.reshape(batch_size, sequence_length, self.n_heads, 3*self.head_dim)\n",
    "        x = x.permute(0, 2, 1, 3)                   # (batch_size, n_heads, max_seq_len, 3*head_dim)\n",
    "        Q, K, V = x.chunk(3, dim=-1)                # (batch_size, n_heads, max_seq_len, head_dim)\n",
    "        x = self.ScaledDotProductAttention(Q, K, V) # (batch_size, n_heads, max_seq_len, head_dim)\n",
    "        x = x.permute(0, 2, 1, 3)                   # (batch_size, max_seq_len, n_heads, head_dim)\n",
    "        x = x.reshape(batch_size, max_seq_len, self.n_heads*self.head_dim)\n",
    "        return self.linear(x)                       # (batch_size, max_seq_len, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b7ac6163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 8, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "input_dim = 1024\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "\n",
    "batch_size = 30\n",
    "sequence_length = 5\n",
    "x = torch.randn( (batch_size, sequence_length, input_dim) )\n",
    "\n",
    "model = MultiheadAttention(input_dim, d_model, num_heads)\n",
    "out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a10ae479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 5, 512])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f5adb9",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea37326",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
